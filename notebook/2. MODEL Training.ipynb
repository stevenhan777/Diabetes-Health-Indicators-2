{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e99e03",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "Import Data and Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a7ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db12a06",
   "metadata": {},
   "source": [
    "Import ML models and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eef8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c1fa4",
   "metadata": {},
   "source": [
    "Define evaluate_model function to evaluate the following metrics on the true and predicted values:\n",
    "* precision - Of all rows the model labeled as positive, what fraction were actually positive\n",
    "* recall - What fraction of actual positives did we correctly identify\n",
    "* f1 score - Calculated as 2*(Precision * Recall)/(Precision + Recall)\n",
    "* ROC_AUC - measures the area under the Receiver Operating Characteristic (ROC) curve.\n",
    "* PR_AUC - (Precision-Recall Area Under the Curve) representing the area under its Precision-Recall curve, which plots precision against recall at various classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2ebd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted, model, X_test):\n",
    "    precision = precision_score(true, predicted , zero_division = 0)\n",
    "    recall = recall_score(true, predicted , zero_division = 0)\n",
    "    f1 = f1_score(true , predicted, zero_division = 0)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(true, y_pred_proba)\n",
    "    return precision, recall, f1, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ec23f",
   "metadata": {},
   "source": [
    "Define function to train and evaluate each model for the scaled data\n",
    "\n",
    "* Logistic Regression - map inputs to a probability between 0 and 1\n",
    "* Logistic Regression with L1 - (LASSO) adds a penalty for the absolute size of coefficients to the model's loss function, forcing less important features' coefficients to become exactly zero\n",
    "* Logistic Regression with L2 - (Ridge) adds a penalty term to the cost function, proportional to the sum of the squared coefficients, to prevent overfitting by shrinking weights towards zero without eliminating them\n",
    "* Support vector classifier - finds the optimal boundary (hyperplane) to separate different classes of data, maximizing the margin between them for accurate classification\n",
    "* K-Neighbors classifier - classifies a new data point by finding its 'k' closest neighbors in the training data and assigning it the most common class (majority vote) among those neighbors\n",
    "* Decision tree - tree-like structure of decisions and their possible consequences to classify data\n",
    "* Rand forest classifier - builds many individual decision trees on random subsets of your data and features, then combines their predictions (via majority vote for classification)\n",
    "* XGB classifier - eXtreme Gradient Boosting algorithm to build highly accurate models by combining many weak decision trees sequentially, with each tree correcting the errors of the last\n",
    "* Catboost classifier - automatically processes categorical features, It builds an ensemble of decision trees, where each new tree corrects the errors of the previous ones\n",
    "* Adaboost classifier - combining the outputs of many simple \"weak\" classifiers\n",
    "* Gradient boost classifier - sequentially combining many simple, \"weak\" models (usually decision trees) to correct the errors of the previous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "\n",
    "        \"Lasso\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "   \n",
    "        \"Ridge\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "\n",
    "        \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "\n",
    "        \"XGBClassifier\": XGBClassifier(), \n",
    "        \n",
    "        \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "\n",
    "        \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "\n",
    "        \"GradientBoosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    " \n",
    "    # Lists to store results\n",
    "    results_train = []\n",
    "    results_test = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    " \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Training predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "        train_precision, train_recall, train_f1, train_roc_auc, train_pr_auc = evaluate_model(\n",
    "            y_train, y_train_pred, model, X_train\n",
    "        )\n",
    "        \n",
    "        # Test predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "        test_precision, test_recall, test_f1, test_roc_auc, test_pr_auc = evaluate_model(\n",
    "            y_test, y_test_pred, model, X_test\n",
    "        )\n",
    "        \n",
    "        # Store training results\n",
    "        results_train.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_train,\n",
    "            'Precision': train_precision,\n",
    "            'Recall': train_recall,\n",
    "            'F1 Score': train_f1,\n",
    "            'ROC AUC': train_roc_auc,\n",
    "            'PR AUC': train_pr_auc\n",
    "        })\n",
    "        \n",
    "        # Store test results\n",
    "        results_test.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_test,\n",
    "            'Precision': test_precision,\n",
    "            'Recall': test_recall,\n",
    "            'F1 Score': test_f1,\n",
    "            'ROC AUC': test_roc_auc,\n",
    "            'PR AUC': test_pr_auc\n",
    "        })\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_train = pd.DataFrame(results_train)\n",
    "    df_test = pd.DataFrame(results_test)\n",
    "    \n",
    "    # Display tables\n",
    "    print(\"=\" * 120)\n",
    "    print(\"TRAINING SET PERFORMANCE\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df_train.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"=\" * 120)\n",
    "    print(\"TEST SET PERFORMANCE\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df_test.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2ce2d",
   "metadata": {},
   "source": [
    "Load in the scaled data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed6b5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('data/scaled_unbalanced_X_train.csv')\n",
    "y_train=pd.read_csv('data/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e816140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202944, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "296e5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.read_csv('data/X_test.csv')\n",
    "y_test=pd.read_csv('data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b948524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50736, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d21586",
   "metadata": {},
   "source": [
    "Run the models with the scaled and unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1853f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "TRAINING SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "        Logistic Regression    0.8476     0.5524  0.1836    0.2756   0.8165  0.4321\n",
      "                      Lasso    0.8476     0.5522  0.1842    0.2763   0.8167  0.4320\n",
      "                      Ridge    0.8476     0.5524  0.1839    0.2759   0.8167  0.4320\n",
      "     K-Neighbors Classifier    0.8726     0.6766  0.3701    0.4785   0.8965  0.5484\n",
      "              Decision Tree    0.9932     0.9987  0.9579    0.9779   0.9998  0.9985\n",
      "   Random Forest Classifier    0.9931     0.9946  0.9617    0.9779   0.9992  0.9971\n",
      "              XGBClassifier    0.8623     0.6709  0.2501    0.3644   0.8500  0.5381\n",
      "     CatBoosting Classifier    0.8651     0.7001  0.2549    0.3737   0.8491  0.5535\n",
      "        AdaBoost Classifier    0.8498     0.5623  0.2202    0.3165   0.8205  0.4502\n",
      "GradientBoosting Classifier    0.8513     0.5808  0.2092    0.3076   0.8259  0.4633\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "TEST SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "        Logistic Regression    0.1565     0.1565  1.0000    0.2707   0.5000  0.1565\n",
      "                      Lasso    0.1565     0.1565  1.0000    0.2707   0.5000  0.1565\n",
      "                      Ridge    0.1565     0.1565  1.0000    0.2707   0.5000  0.1565\n",
      "     K-Neighbors Classifier    0.6817     0.2554  0.5394    0.3466   0.6440  0.2276\n",
      "              Decision Tree    0.4785     0.1012  0.2958    0.1508   0.4041  0.1401\n",
      "   Random Forest Classifier    0.7034     0.2814  0.5757    0.3780   0.6777  0.2653\n",
      "              XGBClassifier    0.7418     0.2639  0.3631    0.3056   0.6118  0.2641\n",
      "     CatBoosting Classifier    0.5192     0.1944  0.6592    0.3003   0.6389  0.2694\n",
      "        AdaBoost Classifier    0.8369     0.4343  0.1381    0.2096   0.7564  0.3358\n",
      "GradientBoosting Classifier    0.3019     0.1671  0.8680    0.2802   0.4676  0.1641\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c4699",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* With regards to ROC_AUC score, Adaboost performs best, and also has the best PR AUC\n",
    "* The Random forest has the best f1 score\n",
    "* The logistic regression models, including Lasso and Ridge, do not perform well, not better than a guess\n",
    "* XGB performs better than GradientBoosting which makes sense as it is an enhanced version of it\n",
    "* Notice trend of higher recall and lower precision because the positive class (having diabetes) is the minority class \n",
    "\n",
    "\n",
    "\n",
    "With an unbalanced dataset we would want to penalize misclassifying the minority class (1 as having diabetes) more heavily - which means we want to maximize precision (minimizing false positives).\n",
    "\n",
    "Adaboost has best precision and best scores overall so would be model of choice for the scaled and unbalanced dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b63b8c",
   "metadata": {},
   "source": [
    "Load in the scaled and Balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb66988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('data/scaled_balanced_X_train.csv')\n",
    "y_train=pd.read_csv('data/scaled_balanced_y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec0673c",
   "metadata": {},
   "source": [
    "Run the classification models on the scaled and Balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "914cc3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "TRAINING SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "        Logistic Regression    0.7464     0.7357  0.7691    0.7520   0.8211  0.7852\n",
      "                      Lasso    0.7464     0.7357  0.7691    0.7520   0.8212  0.7852\n",
      "                      Ridge    0.7464     0.7357  0.7691    0.7520   0.8211  0.7852\n",
      "     K-Neighbors Classifier    0.8601     0.8076  0.9455    0.8711   0.9534  0.9393\n",
      "              Decision Tree    0.9950     0.9990  0.9909    0.9949   0.9999  0.9999\n",
      "   Random Forest Classifier    0.9950     0.9970  0.9929    0.9950   0.9997  0.9997\n",
      "              XGBClassifier    0.8699     0.9102  0.8209    0.8632   0.9476  0.9567\n",
      "     CatBoosting Classifier    0.8784     0.9224  0.8263    0.8717   0.9518  0.9607\n",
      "        AdaBoost Classifier    0.7991     0.7872  0.8197    0.8031   0.8877  0.8929\n",
      "GradientBoosting Classifier    0.8386     0.8396  0.8370    0.8383   0.9254  0.9347\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "TEST SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "        Logistic Regression    0.1565     0.1565  1.0000    0.2707   0.5000  0.1565\n",
      "                      Lasso    0.1565     0.1565  1.0000    0.2707   0.5000  0.1565\n",
      "                      Ridge    0.1565     0.1565  1.0000    0.2707   0.5000  0.1565\n",
      "     K-Neighbors Classifier    0.6126     0.2398  0.6795    0.3545   0.6615  0.2357\n",
      "              Decision Tree    0.5367     0.2091  0.7046    0.3225   0.6050  0.1936\n",
      "   Random Forest Classifier    0.6702     0.2718  0.6592    0.3849   0.7207  0.2936\n",
      "              XGBClassifier    0.5094     0.1947  0.6806    0.3028   0.6120  0.2322\n",
      "     CatBoosting Classifier    0.5867     0.2230  0.6602    0.3334   0.6495  0.2568\n",
      "        AdaBoost Classifier    0.2105     0.1646  0.9927    0.2824   0.7265  0.2818\n",
      "GradientBoosting Classifier    0.2109     0.1644  0.9901    0.2820   0.7193  0.2865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fb71f",
   "metadata": {},
   "source": [
    "Observations of results:\n",
    "\n",
    "* The top 3 models with the best ROC_AUC scores were AdaBoost, Random Forest and gradeint boosting. \n",
    "* These 3 also have the best PR_AUC scores\n",
    "* Once again random forest has the best f1 score\n",
    "* The recall is very high 0.99 for Adaboost and Gradient boost, probably due to model trying to capture the minority class\n",
    "\n",
    "* For the scaled and balanced data, Random forest performs the best overall and would choose as model for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdf165",
   "metadata": {},
   "source": [
    "Define function for models that do not require scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d0911c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_scale_classification(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "\n",
    "        \"XGBClassifier\": XGBClassifier(), \n",
    "        \n",
    "        \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "\n",
    "        \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "\n",
    "        \"GradientBoosting Classifier\": GradientBoostingClassifier()\n",
    "    }\n",
    " \n",
    "    # Lists to store results\n",
    "    results_train = []\n",
    "    results_test = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Training predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "        train_precision, train_recall, train_f1, train_roc_auc, train_pr_auc = evaluate_model(\n",
    "            y_train, y_train_pred, model, X_train\n",
    "        )\n",
    "        \n",
    "        # Test predictions\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "        test_precision, test_recall, test_f1, test_roc_auc, test_pr_auc = evaluate_model(\n",
    "            y_test, y_test_pred, model, X_test\n",
    "        )\n",
    "        \n",
    "        # Store training results\n",
    "        results_train.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_train,\n",
    "            'Precision': train_precision,\n",
    "            'Recall': train_recall,\n",
    "            'F1 Score': train_f1,\n",
    "            'ROC AUC': train_roc_auc,\n",
    "            'PR AUC': train_pr_auc\n",
    "        })\n",
    "        \n",
    "        # Store test results\n",
    "        results_test.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_test,\n",
    "            'Precision': test_precision,\n",
    "            'Recall': test_recall,\n",
    "            'F1 Score': test_f1,\n",
    "            'ROC AUC': test_roc_auc,\n",
    "            'PR AUC': test_pr_auc\n",
    "        })\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_train = pd.DataFrame(results_train)\n",
    "    df_test = pd.DataFrame(results_test)\n",
    "    \n",
    "    # Display tables\n",
    "    print(\"=\" * 120)\n",
    "    print(\"TRAINING SET PERFORMANCE\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df_train.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"=\" * 120)\n",
    "    print(\"TEST SET PERFORMANCE\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df_test.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b1229",
   "metadata": {},
   "source": [
    "Load in the unscaled and unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3531de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('data/unscaled_unbalanced_X_train.csv')\n",
    "y_train=pd.read_csv('data/y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042d234",
   "metadata": {},
   "source": [
    "Run the classification on the unscaled and unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af73e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "TRAINING SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "              Decision Tree    0.9932     0.9987  0.9579    0.9779   0.9998  0.9985\n",
      "   Random Forest Classifier    0.9931     0.9944  0.9619    0.9778   0.9992  0.9971\n",
      "              XGBClassifier    0.8623     0.6709  0.2501    0.3644   0.8500  0.5381\n",
      "     CatBoosting Classifier    0.8651     0.7001  0.2549    0.3737   0.8491  0.5535\n",
      "        AdaBoost Classifier    0.8498     0.5623  0.2202    0.3165   0.8205  0.4502\n",
      "GradientBoosting Classifier    0.8513     0.5808  0.2092    0.3076   0.8259  0.4633\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "TEST SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "              Decision Tree    0.7799     0.3140  0.3430    0.3279   0.5999  0.2119\n",
      "   Random Forest Classifier    0.8429     0.4957  0.2179    0.3027   0.7951  0.3922\n",
      "              XGBClassifier    0.8516     0.5698  0.2117    0.3087   0.8245  0.4500\n",
      "     CatBoosting Classifier    0.8517     0.5716  0.2106    0.3077   0.8271  0.4584\n",
      "        AdaBoost Classifier    0.8510     0.5590  0.2272    0.3231   0.8248  0.4520\n",
      "GradientBoosting Classifier    0.8528     0.5792  0.2186    0.3174   0.8286  0.4593\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_scale_classification(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c49812",
   "metadata": {},
   "source": [
    "Observations on the unscaled vs scaled unbalanced data:\n",
    "\n",
    "* All the boosting models: XGB, CatBoost, AdaBoost and GradientBoosting have similar ROC AUC scores and PR AUC. Gradient boost has the best scores here. These scores are better with the unscaled data, and these models do not require feature scaling. \n",
    "* Note that Gradient boost performs better than XGB, which builds upon gradient boost, which indicated a simpler dataset capture or necessity for hyperparam tuning\n",
    "* Precision is higher than recall for this unscaled data, when the opposite was true when data was scaled. \n",
    "\n",
    "* Considering all the metrics, Adaboost and Gradient boost perform similarily but would choose Gradeint Boost for the unscaled and unbalanced dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38430c29",
   "metadata": {},
   "source": [
    "Load in the unscaled and Balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aae376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('data/unscaled_balanced_X_train.csv')\n",
    "y_train=pd.read_csv('data/unscaled_balanced_y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8146c31",
   "metadata": {},
   "source": [
    "Run the classification models on unscaled and balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db94a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "TRAINING SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "              Decision Tree    0.9950     0.9990  0.9909    0.9949   0.9999  0.9999\n",
      "   Random Forest Classifier    0.9950     0.9970  0.9929    0.9950   0.9997  0.9998\n",
      "              XGBClassifier    0.8704     0.9107  0.8214    0.8637   0.9480  0.9572\n",
      "     CatBoosting Classifier    0.8792     0.9226  0.8280    0.8727   0.9522  0.9610\n",
      "        AdaBoost Classifier    0.8254     0.8219  0.8309    0.8264   0.9147  0.9252\n",
      "GradientBoosting Classifier    0.8517     0.8693  0.8277    0.8480   0.9350  0.9451\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "TEST SET PERFORMANCE\n",
      "========================================================================================================================\n",
      "                      Model  Accuracy  Precision  Recall  F1 Score  ROC AUC  PR AUC\n",
      "              Decision Tree    0.7480     0.2959  0.4424    0.3546   0.6233  0.2200\n",
      "   Random Forest Classifier    0.8209     0.4277  0.4266    0.4272   0.7973  0.3905\n",
      "              XGBClassifier    0.8357     0.4734  0.4419    0.4571   0.8248  0.4525\n",
      "     CatBoosting Classifier    0.8378     0.4798  0.4318    0.4546   0.8264  0.4562\n",
      "        AdaBoost Classifier    0.7873     0.3884  0.6250    0.4791   0.8182  0.4367\n",
      "GradientBoosting Classifier    0.8198     0.4374  0.5297    0.4792   0.8255  0.4513\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_scale_classification(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ce711",
   "metadata": {},
   "source": [
    "Observations of results\n",
    "\n",
    "* All the boosting models: XGB, CatBoost, AdaBoost and GradientBoosting have similar ROC AUC scores and PR AUC. Catboost has the best scores here. These scores are better with the unscaled data, and these models do not require feature scaling. \n",
    "\n",
    "* The precision and recall scores across models are more balanced for the balanced data than unbalanced, which is to be expected.\n",
    "\n",
    "* The F1 score is also better for balanced dataset\n",
    "\n",
    "* Considering all the metrics, Would chooses CatBoost for this dataset\n",
    "\n",
    "* Overall, the precision and recall is more balanced and f1 score is better across models for the unscaled and balanced dataset, so I would choose to balance the dataset. \n",
    "\n",
    "* Overall, ROC_AUC and PR_AUC scores are better for unscaled data, with the models requiring scaling (logistic regression, k neighbors, etc) not performing substantially better than the models that do not require feature scaling. So I will choose to leave data unscaled for the best model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
