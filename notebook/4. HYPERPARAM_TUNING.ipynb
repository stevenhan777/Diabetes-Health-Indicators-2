{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9574c5c2",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning\n",
    "\n",
    "Here I will do hyperparameter tuning on the models on the dataset that is balanced and unscaled as that determined the best dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f08e0",
   "metadata": {},
   "source": [
    "Import required packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7028426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a611e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47bfe9",
   "metadata": {},
   "source": [
    "Define the evaluate_model function as done in Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26857c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted, model, X_test):\n",
    "    precision = precision_score(true, predicted , zero_division = 0)\n",
    "    recall = recall_score(true, predicted , zero_division = 0)\n",
    "    f1 = f1_score(true , predicted, zero_division = 0)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(true, y_pred_proba)\n",
    "    return precision, recall, f1, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e8eea",
   "metadata": {},
   "source": [
    "Define the no_scale_classification function as defined in Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dafe94",
   "metadata": {},
   "source": [
    "These are the following hyperparameters that I use:\n",
    "\n",
    "Decision Tree\n",
    "* 'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "* 'min_samples_split': [2, 5, 10, 20],\n",
    "* 'min_samples_leaf': [1, 2, 4, 8],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'class_weight': ['balanced', None]\n",
    "* \n",
    "\n",
    "Random forest\n",
    "* \n",
    "* \n",
    "\n",
    "XGBoost\n",
    "* \n",
    "* \n",
    "\n",
    "CatBoost\n",
    "* \n",
    "* \n",
    "\n",
    "AdaBoost\n",
    "* \n",
    "* \n",
    "\n",
    "Gradient Boosting\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbad6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_scale_classification_hyperparam_tuning(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Models with hyperparameter tuning\n",
    "    tuned_models = {\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeClassifier(random_state=42),\n",
    "            \"params\": {\n",
    "                'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10, 20],\n",
    "                'min_samples_leaf': [1, 2, 4, 8],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'class_weight': ['balanced', None]\n",
    "            }\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestClassifier(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2'],\n",
    "                'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "                'bootstrap': [True, False]\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "            \"params\": {\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_depth': [3, 5, 7, 9],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                'subsample': [0.6, 0.8, 1.0],\n",
    "                'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                'gamma': [0, 0.1, 0.5, 1],\n",
    "                'min_child_weight': [1, 3, 5],\n",
    "                'scale_pos_weight': [1, 2, 3]\n",
    "            }\n",
    "        },\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostClassifier(random_state=42, verbose=False),\n",
    "            \"params\": {\n",
    "                'iterations': [50, 100, 200, 300],\n",
    "                'depth': [4, 6, 8, 10],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                'l2_leaf_reg': [1, 3, 5, 7],\n",
    "                'border_count': [32, 64, 128],\n",
    "                'class_weights': [[1, 1], [1, 2], [1, 3]]\n",
    "            }\n",
    "        },\n",
    "        \"AdaBoost\": {\n",
    "            \"model\": AdaBoostClassifier(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "                'algorithm': ['SAMME', 'SAMME.R']\n",
    "            }\n",
    "        },\n",
    "        \"GradientBoosting\": {\n",
    "            \"model\": GradientBoostingClassifier(random_state=42),\n",
    "            \"params\": {\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_depth': [3, 5, 7, 9],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                'subsample': [0.6, 0.8, 1.0],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'loss': ['log_loss', 'exponential'],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Train models with hyperparameter tuning\n",
    "    print(\"\\nTraining models with hyperparameter tuning...\")\n",
    "    for model_name, model_info in tuned_models.items():\n",
    "        print(f\"  - {model_name} (this may take a while...)\")\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"model\"],\n",
    "            param_distributions=model_info[\"params\"],\n",
    "            n_iter=50,\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        train_precision, train_recall, train_f1, train_roc_auc, train_pr_auc = evaluate_model(\n",
    "            y_train, y_train_pred, best_model, X_train\n",
    "        )\n",
    "        test_precision, test_recall, test_f1, test_roc_auc, test_pr_auc = evaluate_model(\n",
    "            y_test, y_test_pred, best_model, X_test\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Train Accuracy': accuracy_train,\n",
    "            'Test Accuracy': accuracy_test,\n",
    "            'Test Precision': test_precision,\n",
    "            'Test Recall': test_recall,\n",
    "            'Test F1': test_f1,\n",
    "            'Test ROC AUC': test_roc_auc,\n",
    "            'Test PR AUC': test_pr_auc,\n",
    "            'Tuned': 'Yes',\n",
    "            'Best Params': str(random_search.best_params_)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Display main results table\n",
    "    print(\"\\n\" + \"=\" * 150)\n",
    "    print(\"MODEL COMPARISON - ALL METRICS\")\n",
    "    print(\"=\" * 150)\n",
    "    display_df = df_results.drop('Best Params', axis=1)\n",
    "    print(display_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    \n",
    "    # Display best parameters for tuned models\n",
    "    print(\"\\n\" + \"=\" * 150)\n",
    "    print(\"BEST HYPERPARAMETERS FOR TUNED MODELS\")\n",
    "    print(\"=\" * 150)\n",
    "    tuned_df = df_results[df_results['Tuned'] == 'Yes'][['Model', 'Best Params']]\n",
    "    for idx, row in tuned_df.iterrows():\n",
    "        print(f\"\\n{row['Model']}:\")\n",
    "        params = eval(row['Best Params'])\n",
    "        for param, value in params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "    \n",
    "    # Display sorted by Test ROC AUC\n",
    "    print(\"\\n\" + \"=\" * 150)\n",
    "    print(\"TOP 5 MODELS BY TEST ROC AUC\")\n",
    "    print(\"=\" * 150)\n",
    "    top_models = df_results.nlargest(5, 'Test ROC AUC')[['Model', 'Test Accuracy', 'Test ROC AUC', 'Test F1', 'Tuned']]\n",
    "    print(top_models.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f91a2d",
   "metadata": {},
   "source": [
    "Load in the unscaled and balanced dataset, with final feature selection performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b83e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('data/final_X_train.csv')\n",
    "y_train=pd.read_csv('data/unscaled_balanced_y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880e80f",
   "metadata": {},
   "source": [
    "Load in the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c04199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.read_csv('data/final_X_test.csv')\n",
    "y_test=pd.read_csv('data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afd262",
   "metadata": {},
   "source": [
    "Run the hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8222e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models with hyperparameter tuning...\n",
      "  - Decision Tree (this may take a while...)\n",
      "  - Random Forest (this may take a while...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - XGBoost (this may take a while...)\n",
      "  - CatBoost (this may take a while...)\n",
      "  - AdaBoost (this may take a while...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 40 is smaller than n_iter=50. Running 40 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - GradientBoosting (this may take a while...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================================================================================\n",
      "MODEL COMPARISON - ALL METRICS\n",
      "======================================================================================================================================================\n",
      "           Model  Train Accuracy  Test Accuracy  Test Precision  Test Recall  Test F1  Test ROC AUC  Test PR AUC Tuned\n",
      "   Decision Tree          0.8359         0.7962          0.3934       0.5571   0.4611        0.7938       0.4030   Yes\n",
      "   Random Forest          0.9737         0.8205          0.4276       0.4342   0.4309        0.8016       0.4065   Yes\n",
      "         XGBoost          0.8636         0.8383          0.4819       0.4384   0.4591        0.8252       0.4577   Yes\n",
      "        CatBoost          0.8655         0.8387          0.4833       0.4420   0.4618        0.8264       0.4590   Yes\n",
      "        AdaBoost          0.8455         0.8166          0.4300       0.5281   0.4740        0.8205       0.4452   Yes\n",
      "GradientBoosting          0.8672         0.8359          0.4745       0.4489   0.4614        0.8245       0.4557   Yes\n",
      "\n",
      "======================================================================================================================================================\n",
      "BEST HYPERPARAMETERS FOR TUNED MODELS\n",
      "======================================================================================================================================================\n",
      "\n",
      "Decision Tree:\n",
      "  - min_samples_split: 2\n",
      "  - min_samples_leaf: 8\n",
      "  - max_features: None\n",
      "  - max_depth: 15\n",
      "  - criterion: gini\n",
      "  - class_weight: None\n",
      "\n",
      "Random Forest:\n",
      "  - n_estimators: 300\n",
      "  - min_samples_split: 5\n",
      "  - min_samples_leaf: 2\n",
      "  - max_features: sqrt\n",
      "  - max_depth: None\n",
      "  - class_weight: balanced_subsample\n",
      "  - bootstrap: False\n",
      "\n",
      "XGBoost:\n",
      "  - subsample: 1.0\n",
      "  - scale_pos_weight: 1\n",
      "  - n_estimators: 200\n",
      "  - min_child_weight: 1\n",
      "  - max_depth: 5\n",
      "  - learning_rate: 0.2\n",
      "  - gamma: 0.1\n",
      "  - colsample_bytree: 0.6\n",
      "\n",
      "CatBoost:\n",
      "  - learning_rate: 0.1\n",
      "  - l2_leaf_reg: 5\n",
      "  - iterations: 300\n",
      "  - depth: 6\n",
      "  - class_weights: [1, 1]\n",
      "  - border_count: 128\n",
      "\n",
      "AdaBoost:\n",
      "  - n_estimators: 300\n",
      "  - learning_rate: 1.0\n",
      "  - algorithm: SAMME.R\n",
      "\n",
      "GradientBoosting:\n",
      "  - subsample: 0.8\n",
      "  - n_estimators: 300\n",
      "  - min_samples_split: 2\n",
      "  - min_samples_leaf: 1\n",
      "  - max_features: None\n",
      "  - max_depth: 5\n",
      "  - loss: log_loss\n",
      "  - learning_rate: 0.1\n",
      "\n",
      "======================================================================================================================================================\n",
      "TOP 5 MODELS BY TEST ROC AUC\n",
      "======================================================================================================================================================\n",
      "           Model  Test Accuracy  Test ROC AUC  Test F1 Tuned\n",
      "        CatBoost         0.8387        0.8264   0.4618   Yes\n",
      "         XGBoost         0.8383        0.8252   0.4591   Yes\n",
      "GradientBoosting         0.8359        0.8245   0.4614   Yes\n",
      "        AdaBoost         0.8166        0.8205   0.4740   Yes\n",
      "   Random Forest         0.8205        0.8016   0.4309   Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Test ROC AUC</th>\n",
       "      <th>Test PR AUC</th>\n",
       "      <th>Tuned</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.835910</td>\n",
       "      <td>0.796220</td>\n",
       "      <td>0.393384</td>\n",
       "      <td>0.557109</td>\n",
       "      <td>0.461146</td>\n",
       "      <td>0.793831</td>\n",
       "      <td>0.402966</td>\n",
       "      <td>Yes</td>\n",
       "      <td>{'min_samples_split': 2, 'min_samples_leaf': 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.973710</td>\n",
       "      <td>0.820482</td>\n",
       "      <td>0.427632</td>\n",
       "      <td>0.434202</td>\n",
       "      <td>0.430892</td>\n",
       "      <td>0.801592</td>\n",
       "      <td>0.406459</td>\n",
       "      <td>Yes</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.863630</td>\n",
       "      <td>0.838340</td>\n",
       "      <td>0.481933</td>\n",
       "      <td>0.438358</td>\n",
       "      <td>0.459114</td>\n",
       "      <td>0.825184</td>\n",
       "      <td>0.457683</td>\n",
       "      <td>Yes</td>\n",
       "      <td>{'subsample': 1.0, 'scale_pos_weight': 1, 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.865475</td>\n",
       "      <td>0.838714</td>\n",
       "      <td>0.483338</td>\n",
       "      <td>0.442010</td>\n",
       "      <td>0.461751</td>\n",
       "      <td>0.826403</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>Yes</td>\n",
       "      <td>{'learning_rate': 0.1, 'l2_leaf_reg': 5, 'iter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.845525</td>\n",
       "      <td>0.816560</td>\n",
       "      <td>0.429977</td>\n",
       "      <td>0.528145</td>\n",
       "      <td>0.474032</td>\n",
       "      <td>0.820547</td>\n",
       "      <td>0.445158</td>\n",
       "      <td>Yes</td>\n",
       "      <td>{'n_estimators': 300, 'learning_rate': 1.0, 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.867205</td>\n",
       "      <td>0.835935</td>\n",
       "      <td>0.474511</td>\n",
       "      <td>0.448936</td>\n",
       "      <td>0.461369</td>\n",
       "      <td>0.824529</td>\n",
       "      <td>0.455708</td>\n",
       "      <td>Yes</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 300, 'min_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Train Accuracy  Test Accuracy  Test Precision  \\\n",
       "0     Decision Tree        0.835910       0.796220        0.393384   \n",
       "1     Random Forest        0.973710       0.820482        0.427632   \n",
       "2           XGBoost        0.863630       0.838340        0.481933   \n",
       "3          CatBoost        0.865475       0.838714        0.483338   \n",
       "4          AdaBoost        0.845525       0.816560        0.429977   \n",
       "5  GradientBoosting        0.867205       0.835935        0.474511   \n",
       "\n",
       "   Test Recall   Test F1  Test ROC AUC  Test PR AUC Tuned  \\\n",
       "0     0.557109  0.461146      0.793831     0.402966   Yes   \n",
       "1     0.434202  0.430892      0.801592     0.406459   Yes   \n",
       "2     0.438358  0.459114      0.825184     0.457683   Yes   \n",
       "3     0.442010  0.461751      0.826403     0.459016   Yes   \n",
       "4     0.528145  0.474032      0.820547     0.445158   Yes   \n",
       "5     0.448936  0.461369      0.824529     0.455708   Yes   \n",
       "\n",
       "                                         Best Params  \n",
       "0  {'min_samples_split': 2, 'min_samples_leaf': 8...  \n",
       "1  {'n_estimators': 300, 'min_samples_split': 5, ...  \n",
       "2  {'subsample': 1.0, 'scale_pos_weight': 1, 'n_e...  \n",
       "3  {'learning_rate': 0.1, 'l2_leaf_reg': 5, 'iter...  \n",
       "4  {'n_estimators': 300, 'learning_rate': 1.0, 'a...  \n",
       "5  {'subsample': 0.8, 'n_estimators': 300, 'min_s...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_scale_classification_hyperparam_tuning(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57565ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================================================\n",
    "# MODEL COMPARISON - ALL METRICS\n",
    "# ======================================================================================================================================================\n",
    "#            Model  Train Accuracy  Test Accuracy  Test Precision  Test Recall  Test F1  Test ROC AUC  Test PR AUC Tuned\n",
    "#    Decision Tree          0.8359         0.7962          0.3934       0.5571   0.4611        0.7938       0.4030   Yes\n",
    "#    Random Forest          0.9737         0.8205          0.4276       0.4342   0.4309        0.8016       0.4065   Yes\n",
    "#          XGBoost          0.8636         0.8383          0.4819       0.4384   0.4591        0.8252       0.4577   Yes\n",
    "#         CatBoost          0.8655         0.8387          0.4833       0.4420   0.4618        0.8264       0.4590   Yes\n",
    "#         AdaBoost          0.8455         0.8166          0.4300       0.5281   0.4740        0.8205       0.4452   Yes\n",
    "# GradientBoosting          0.8672         0.8359          0.4745       0.4489   0.4614        0.8245       0.4557   Yes\n",
    "\n",
    "# ======================================================================================================================================================\n",
    "# BEST HYPERPARAMETERS FOR TUNED MODELS\n",
    "# ======================================================================================================================================================\n",
    "\n",
    "# Decision Tree:\n",
    "#   - min_samples_split: 2\n",
    "#   - min_samples_leaf: 8\n",
    "#   - max_features: None\n",
    "#   - max_depth: 15\n",
    "#   - criterion: gini\n",
    "#   - class_weight: None\n",
    "\n",
    "# Random Forest:\n",
    "#   - n_estimators: 300\n",
    "#   - min_samples_split: 5\n",
    "#   - min_samples_leaf: 2\n",
    "#   - max_features: sqrt\n",
    "#   - max_depth: None\n",
    "#   - class_weight: balanced_subsample\n",
    "#   - bootstrap: False\n",
    "\n",
    "# XGBoost:\n",
    "#   - subsample: 1.0\n",
    "#   - scale_pos_weight: 1\n",
    "#   - n_estimators: 200\n",
    "#   - min_child_weight: 1\n",
    "#   - max_depth: 5\n",
    "#   - learning_rate: 0.2\n",
    "#   - gamma: 0.1\n",
    "#   - colsample_bytree: 0.6\n",
    "\n",
    "# CatBoost:\n",
    "#   - learning_rate: 0.1\n",
    "#   - l2_leaf_reg: 5\n",
    "#   - iterations: 300\n",
    "#   - depth: 6\n",
    "#   - class_weights: [1, 1]\n",
    "#   - border_count: 128\n",
    "\n",
    "# AdaBoost:\n",
    "#   - n_estimators: 300\n",
    "#   - learning_rate: 1.0\n",
    "#   - algorithm: SAMME.R\n",
    "\n",
    "# GradientBoosting:\n",
    "#   - subsample: 0.8\n",
    "#   - n_estimators: 300\n",
    "#   - min_samples_split: 2\n",
    "#   - min_samples_leaf: 1\n",
    "#   - max_features: None\n",
    "#   - max_depth: 5\n",
    "#   - loss: log_loss\n",
    "#   - learning_rate: 0.1\n",
    "\n",
    "# ======================================================================================================================================================\n",
    "# TOP 5 MODELS BY TEST ROC AUC\n",
    "# ======================================================================================================================================================\n",
    "#            Model  Test Accuracy  Test ROC AUC  Test F1 Tuned\n",
    "#         CatBoost         0.8387        0.8264   0.4618   Yes\n",
    "#          XGBoost         0.8383        0.8252   0.4591   Yes\n",
    "# GradientBoosting         0.8359        0.8245   0.4614   Yes\n",
    "#         AdaBoost         0.8166        0.8205   0.4740   Yes\n",
    "#    Random Forest         0.8205        0.8016   0.4309   Yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
